#!/usr/bin/env python3
# ==============================================================================
# Ù…Ù„Ù: db_optimizer.py (Ù…ØµÙØ­ÙÙ‘Ø­)
# Ø§Ù„ÙˆØµÙ: Ø£Ø¯Ø§Ø© ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª - Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø§Ø±Ø³ CONCURRENTLY Ø®Ø§Ø±Ø¬ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª + Ø¥ØµÙ„Ø§Ø­ ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡
# ==============================================================================

import os
import psycopg2
from psycopg2.extras import DictCursor
from urllib.parse import urlparse
import logging
import time

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def get_db_config():
    DATABASE_URL = os.environ.get('DATABASE_URL')
    if not DATABASE_URL:
        raise RuntimeError('DATABASE_URL not set')
    r = urlparse(DATABASE_URL)
    return {
        'user': r.username,
        'password': r.password,
        'host': r.hostname,
        'port': r.port,
        'dbname': r.path[1:],
    }


def connect(autocommit=False):
    try:
        cfg = get_db_config()
        conn = psycopg2.connect(**cfg)
        conn.set_session(autocommit=autocommit)
        return conn
    except Exception as e:
        logger.error(f"Failed to connect to DB: {e}")
        return None


def check_index_exists(conn, index_name: str) -> bool:
    with conn.cursor(cursor_factory=DictCursor) as cur:
        cur.execute("SELECT 1 FROM pg_indexes WHERE indexname = %s", (index_name,))
        return cur.fetchone() is not None


def create_index_concurrently(sql_stmt: str, index_name: str) -> bool:
    """ÙŠÙ†ÙØ° CREATE INDEX CONCURRENTLY Ø®Ø§Ø±Ø¬ Ø£ÙŠ Ù…Ø¹Ø§Ù…Ù„Ø© (autocommit)."""
    try:
        conn = connect(autocommit=True)
        try:
            with conn.cursor(cursor_factory=DictCursor) as cur:
                if check_index_exists(conn, index_name):
                    logger.info(f"âœ… Ø§Ù„ÙÙ‡Ø±Ø³ {index_name} Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ù„ÙØ¹Ù„")
                    return True
                logger.info(f"ğŸ”„ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ {index_name}...")
                t0 = time.time()
                cur.execute(sql_stmt)
                logger.info(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ {index_name} ÙÙŠ {(time.time()-t0):.2f}s")
                return True
        finally:
            conn.close()
    except Exception as e:
        logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ {index_name}: {e}")
        return False


def enable_pg_trgm():
    try:
        conn = connect(autocommit=True)
        try:
            with conn.cursor() as cur:
                cur.execute("CREATE EXTENSION IF NOT EXISTS pg_trgm")
                logger.info("âœ… ØªÙ… ØªÙØ¹ÙŠÙ„ Ø§Ù…ØªØ¯Ø§Ø¯ pg_trgm Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ø³Ù†")
        finally:
            conn.close()
    except Exception as e:
        logger.warning(f"âš ï¸ Ù„Ù… ÙŠØªÙ… ØªÙØ¹ÙŠÙ„ pg_trgm: {e}")


def check_database_performance():
    logger.info("ğŸ“Š ÙØ­Øµ Ø£Ø¯Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    try:
        conn = connect(autocommit=True)
        try:
            with conn.cursor(cursor_factory=DictCursor) as cur:
                # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„ØµØ­ÙŠØ­Ø© Ø¹Ø¨Ø± pg_stat_user_tables + pg_class
                cur.execute(
                    """
                    SELECT c.relname AS table_name,
                           st.n_live_tup AS live_rows,
                           st.n_dead_tup AS dead_rows
                    FROM pg_stat_user_tables st
                    JOIN pg_class c ON c.oid = st.relid
                    ORDER BY st.n_live_tup DESC
                    """
                )
                for row in cur.fetchall()[:10]:
                    logger.info(f"   ğŸ“‹ {row['table_name']}: {row['live_rows']:,} Ø­ÙŠ, {row['dead_rows']:,} Ù…ÙŠØª")

                # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø¹Ø¨Ø± pg_stat_user_indexes + pg_class
                cur.execute(
                    """
                    SELECT c.relname AS table_name,
                           i.relname AS index_name,
                           idx.idx_tup_read,
                           idx.idx_tup_fetch
                    FROM pg_stat_user_indexes idx
                    JOIN pg_class c ON c.oid = idx.relid
                    JOIN pg_class i ON i.oid = idx.indexrelid
                    ORDER BY idx.idx_tup_read DESC
                    LIMIT 10
                    """
                )
                for row in cur.fetchall():
                    logger.info(f"   ğŸ” {row['index_name']} Ø¹Ù„Ù‰ {row['table_name']}: {row['idx_tup_read']:,} Ù‚Ø±Ø§Ø¡Ø©")
        finally:
            conn.close()
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø§Ù„Ø£Ø¯Ø§Ø¡: {e}")


def optimize_database_performance():
    logger.info("ğŸš€ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    enable_pg_trgm()

    indexes = [
        # video_archive
        ("CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_video_archive_category_id ON video_archive(category_id)", "idx_video_archive_category_id"),
        ("CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_video_archive_view_count_desc ON video_archive(view_count DESC)", "idx_video_archive_view_count_desc"),
        ("CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_video_archive_upload_date_desc ON video_archive(upload_date DESC)", "idx_video_archive_upload_date_desc"),
        ("CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_video_archive_message_id ON video_archive(message_id)", "idx_video_archive_message_id"),
        ("CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_video_archive_caption_trgm ON video_archive USING gin (caption gin_trgm_ops)", "idx_video_archive_caption_trgm"),
        ("CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_video_archive_filename_trgm ON video_archive USING gin (file_name gin_trgm_ops)", "idx_video_archive_filename_trgm"),
        ("CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_video_archive_grouping_key ON video_archive(grouping_key)", "idx_video_archive_grouping_key"),
        # categories
        ("CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_categories_parent_id ON categories(parent_id)", "idx_categories_parent_id"),
        ("CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_categories_name ON categories(name)", "idx_categories_name"),
        ("CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_categories_full_path ON categories(full_path)", "idx_categories_full_path"),
        # user_favorites
        ("CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_user_favorites_user_id ON user_favorites(user_id)", "idx_user_favorites_user_id"),
        ("CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_user_favorites_video_id ON user_favorites(video_id)", "idx_user_favorites_video_id"),
        ("CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_user_favorites_date_added_desc ON user_favorites(date_added DESC)", "idx_user_favorites_date_added_desc"),
        # user_history
        ("CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_user_history_user_id ON user_history(user_id)", "idx_user_history_user_id"),
        ("CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_user_history_last_watched_desc ON user_history(last_watched DESC)", "idx_user_history_last_watched_desc"),
        # video_ratings
        ("CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_video_ratings_video_id ON video_ratings(video_id)", "idx_video_ratings_video_id"),
        ("CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_video_ratings_user_id ON video_ratings(user_id)", "idx_video_ratings_user_id"),
        # bot_users
        ("CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_bot_users_join_date_desc ON bot_users(join_date DESC)", "idx_bot_users_join_date_desc"),
        ("CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_bot_users_username ON bot_users(username)", "idx_bot_users_username"),
        # user_states
        ("CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_user_states_state ON user_states(state)", "idx_user_states_state"),
    ]

    ok = 0
    fail = 0
    for sql_stmt, idx_name in indexes:
        if create_index_concurrently(sql_stmt, idx_name):
            ok += 1
        else:
            fail += 1
        time.sleep(0.2)

    # ANALYZE Ø¨Ø¹Ø¯ Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡
    try:
        conn = connect(autocommit=True)
        try:
            with conn.cursor() as cur:
                logger.info("ğŸ“Š ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
                cur.execute("ANALYZE")
                logger.info("âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        finally:
            conn.close()
    except Exception as e:
        logger.warning(f"âš ï¸ Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª: {e}")

    logger.info("\nğŸ‰ ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† ØªØ­Ø³ÙŠÙ† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª!")
    logger.info(f"âœ… ÙÙ‡Ø§Ø±Ø³ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§ Ø¨Ù†Ø¬Ø§Ø­: {ok}")
    logger.info(f"âŒ ÙÙ‡Ø§Ø±Ø³ ÙØ´Ù„ Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§: {fail}")
    logger.info(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {len(indexes)}")
    return ok > 0


optimization_lock = False

def main():
    global optimization_lock
    if optimization_lock:
        logger.warning("Optimization already running")
        return False
        
    optimization_lock = True
    try:
        logger.info("ğŸ”§ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        check_database_performance()
        result = optimize_database_performance()
        logger.info("\nğŸ“Š ÙØ­Øµ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ø³ÙŠÙ†:")
        check_database_performance()
        return result
    except Exception as e:
        logger.error(f"Optimization failed: {e}")
        return False
    finally:
        optimization_lock = False

if __name__ == "__main__":
    main()
